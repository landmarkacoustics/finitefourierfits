---
title: "lab-notebook"
output: rmarkdown::html_vignette
description: >
  A lab notebook for finitefourierfits. This is basically a dev diary.
vignette: >
  %\VignetteIndexEntry{lab-notebook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(finitefourierfits)
set.seed(42)
```
# 2020 July 13-15

I developed the basic algorithm ideas over these days. The goal is to take a
bivariate relation that seems to be functional and fit a partial Fourier basis
to it. Fewer terms will mean less computational complexity and more smoothness.

The steps that I came up with are

1. pad the response variable with zeros so that its length is more than doubled
and a power of two.

1. find a mapping, `u(x)`, from the causal variable to angular frequency.

1. compute the DFT of the padded variable with `fft`

   1. use `Mod` to compute `a`, the amplitudes of each term in the DFT.
   
   1. use `Arg` to compute `p`, the phases of each term in the DFT.

1. choose a limited number of terms to include

   1. I started by trying to quantify the proportion of information captured by
   each term
   
   1. I ended up just using a fixed number of terms
   
1. build a formula to use with `nls`

   1. the l.h.s. is just the response variable 

   1. the r.h.s is built from the DFT's terms
   
      1. if the CF term is included, just add a term called `b`.
      
      1. if term `i` is included, add `a{i} * cos({i-1} * u(x) + {p[i]})`
      
         1. if `i=3` & `p[3] =-1.1` then the term is `a3 * cos(2 * u(x) - 1.1)`
         
   1. it turned out that the phases didn't change much when I let them vary,
   so I decided to set them as constants. This may cause problems with
   numerical precision because they are actually pasted into the formulae.
   
1. run the `nls` fit to find the values of `b` (if necessary) and the `a_i`s

1. the resulting function is the approximation.

Here's what it looked like after I plugged away for a while. Bonus: I won't get
annoying messages about commented code after I put it here!

```{r, eval=FALSE, fig.dim=c(5, 5), out.width="95%"}
x <- seq(-2, 2, length.out=401)
N <- length(x)
fft.size <- 4 * nextn(N, 2)
half <- floor(fft.size / 2)
omegas <- finitefourierfits::omegas(fft.size)
b <- x[1]
Hz <- (N-1)/(x[N]-b)
d.omega <- (fft.size-1)/omegas[fft.size]
m <- Hz / d.omega
u <- function(x) {m*(x-b)}
plot(x, u(x), las=1, pch=16, col="blue",
     xlab=expression(x), ylab=expression(u(x)))
y <- (x-rnorm(1))*(x-rnorm(1))*(x-rnorm(1))
mu <- mean(y)
S <- fft(c(y - mu, rep(0, fft.size-N)))
a <- finitefourierfits::.amplitudes(S)
p <- finitefourierfits::.phases(S)
mag.order <- order(a, decreasing=TRUE)[1:10]
all.terms <- build.term.list(mag.order, p)
all.starts <- a[mag.order]
names(all.starts) <- names(all.terms)
all.starts
step.fits <- list()
for(i in 1:10) {
    ix <- 1:i
    tmp <- tryCatch(nls(formula(paste("y-mu ~",
                                      paste(all.terms[ix], collapse=" + "))),
                        data.frame(w=u(x), y=y),
                        start=all.starts[ix]),
                    error=function(e) NULL)
    if(!is.null(tmp)){
        step.fits <- append(step.fits, list(tmp))
    }
}
if (length(step.fits)) {
    scores <- sapply(step.fits, BIC)
    tops <- match(min(scores), scores)
    plot(x, y,
	     las=1, col="blue",
		 xlab=expression(x), ylab=expression(f(x)))
    lines(x, predict(step.fits[[tops]])+mu, col="orange", lwd=2)
	summary(step.fits[[tops]])
}
```

# 2020 July 16

This was the first day that I started working on this project as a package.
I am using Hadley Wickham's [e-book](http://r-pkgs.had.co.nz) about making R
packages as a guide. The first hour was spent installing the tools. Wickham's
package, `devtools`, depends on the library `pandoc` and the development
libraries for `curl` and `openssl`. The error messages from the failed install
ended up leading me to them. In general, the e-book is not up-to-date with the
CRAN package including, ironically, creating vignettes. I actually didn't write
any vignette on this day at all. I did create roxygen documentation for some
functions, made progress with the linter `lintr`, and wrote some unit tests as
well.


# 2020 July 17

## Getting vignettes to work

Today is when I actually started writing this vignette. One guide that I am
following is Robert M. Flight's
[blog post](http://rmflight.github.io/posts/2014/07/vignetteAnalysis.html)
about using vignettes to document the process of creating a package.

**I DIDN'T USE `devtools::check` YESTERDAY. IT'S AWESOME.**

Don't use `@examples` in internal functions!

*AHA!* This [e-book](http://r-pkgs.had.co.nz) is out-of-date because they broke
`devtools` into sub-packages for DRY reasons. The `usethis` package (which
is now a dependency of `devtools` is now the namespace for some of the
functions. [This site](https://www.tidyverse.org/blog/2018/10/devtools-2-0-0/)
has details.

There are a lot of problems in getting Rmarkdown to work. The markdown renderer
libraries `pandoc` **AND** `pandoc-citeproc` need to be installed. Also, some
stage of the renderer tries to work with EMACS temporary files and crashes, so
you can't have unsaved changes to any of the vignette markdown files. Maybe
this is a feature, not a bug??

## Linting

I can't seem to get the `devtools::lint` to listen to my customizations. This
does work, though:
```{r eval=FALSE}
my.linters <- lintr::with_defaults(
  object_name_linter=lintr::object_name_linter(styles="dotted.case"),
  infix_spaces_linter=NULL,
  commented_code_linter=NULL
)
lintr::lint_package(linters=my.linters)
```
My code style will depart from Wickham's in three ways:

1. Dotted case names are *de rigeur* b/c ESS shortcuts undercore to assignment

1. I will use Python code style for spaces around infix operators.

1. Sometimes I have commented code as I work. I don't need to be reminded.

## Actually developing the package

Now that I have a workflow up and running, I need to keep working on the code.
Today's goal is to implement a more object-oriented interface to creating the
fits. I need to encapsulate the steps because my goal is an interface like:
```{r eval=FALSE}
my.fit <- fffit(my.data$x, my.data$y, model.selector=BIC)
my.fit$n.terms
coef(my.fit)
```
My initial attempts at a monolithic function were not wildly successful. In
particular, it is hard to debug. This is one of the problems with R, though.
Pass-by-value means that there are nasty performance consequences to breaking
an algorithm into component steps. I'm going to make it an object at some
point, anyway, but it feels weird to break up what is essentially an ctor
into steps.

One quick gotcha is that tryCatch hides errors. I shouldn't activate it until
I am ***SURE*** that everything eles works.

I think I got it working:

```{r, fig.dim=c(6, 6), out.width="95%"}
x <- seq(-2, 2, length.out=401)
y <- 10*(x/2)^2 + rnorm(length(x))
tmp <- finitefourierfits::fffit(x, y)
summary(tmp)
plot(x, y, las=1, col="blue", pch=16, xlab=expression(x), ylab=expression(f(x)))
lines(x, predict(tmp), col="orange", lwd=2)
```

# 2020 July 20

The next step is to turn this stuff into an S3 object interface. Initially,
I thought that I wanted to just save the best fit, but now I think I want to
save all of the fits that work and the index of the best fit.

The functions that I want to define for this

predict
~ Predicted values for some new data from the best (or a specified) model, or
the fitted values from the best (or specified) model.

~~plot~~
~ Draw a scatter plot with the data and a fit through it. This is actually too
hard to do right now.

coef
~ The coefficients of the best model, or optionally an indexed fit

formula
~ The formula of the best model, or optionally an indexed fit

summary
~ Summarize the best fit, or optionally an indexed fit

anova
~ compare all of the fits in order of degrees of freedom and likelihood

### side note about configuring `lintr`
You can avoid re-creating `my.linters` every time you lint with a  `.linter`
configuration file in the project directory. It irritates `devtools::check`,
though.

## installing on a Windows machine
__MAKE SURE THAT `R` is up to date__

## More checking

I'm going to see if the parameters of the fits actually match the parameters
of input functions that are sums of cosines.

```{r, fig.dim=c(6, 6), out.width="95%"}
x <- seq(-2, 2, length.out=401)
f <- function(x, a=c(1, 1), b=c(0, 1)){
  a[1]*cos(a[2]*2*pi*x) + b[1]*cos(b[2]*2*pi*x)
}
phi <- c(0.6, 2.5)
y <- 5*f(x) + rnorm(length(x))
yfit <- fffit(x, y)
z <- 5*f(x, c(1, phi[1]), c(1, phi[2])) + rnorm(length(x))
zfit <- fffit(x, z)
plot(x, y,
     las=1,
     ylim=10*c(-1, 2),
     xlab=expression(x), ylab=expression(f(x)),
     pch=1, col="blue")
points(x, z,
      pch=1, col="orange")
lines(x, predict(yfit), col="blue", lwd=2)
lines(x, predict(zfit), col="orange", lwd=2)
legend("topright",
       c(expression(cos(2*pi*x)),
         bquote(cos(.(2*phi[1])*pi*x) + cos(.(2*phi[2])*pi*x))),
       pch=1,
       lty=1,
       col=c("blue", "orange"))
summary(yfit)
summary(zfit)
```

The fits are pretty amazing, but it's hard to tell what the actual function
of x is that it represents! That's something to work on tomorrow, for totes.

# 2020 July 21

## What are the actual parameters that I'm trying to capture?

My previous attempts have involved transforming the original predictor variable
into angular frequency in radians. I was calling the linear transformation 'u'.
I also assigned a lot of variables as constants, especially the phases.

Noodling around with the results of my first analyses showed that the fits are
good, but the coefficients that I found weren't useful. Basically, I was
creating fits of the form $y = \sum_i{a_{i} \cos((i-1)u(x) + p_{i}})$, where
$u(x) = 2\pi x N_s / K$, where $N_s$ is the sample size, $K$ is the FFT size,
and $i$ is the index of a term in the DFT.

What I really want is $y = \sum_i{a_{i} \cos(2 \pi f_i x + p_i)}$. I should
have the DFT inform my _inital estimates_ for the coefficients, **not**
constants.

# 2020 July 22

I made progress yesterday on changing how the results are structured and
reported. It's definitely headed toward a more object-oriented approach, and
that's going to be much better.

Today I refactored term construction using test-driven development and it was
very satisfying and productive. The REPEATED interruptions from the family were
easier to deal with because I could pick up at the unit test that wasn't
working.

### Weak typing in R

Usually, I prefer to use `NULL` as my marker for "the default behavior didn't
happen, but that's OK because there's an anticipated alternative procedure."
That did NOT work today when in the `fffterm` constructor. I was trying to
convert vectors with `name` attributes to lists with items whose names come
from that attribute. R could handle that when the vector contained no `NULL`s,
but not when they were present.

# 2020 July 23

The beginning of the day plan is to keep up with the test-driven development. I
should be on `fffit` now!

## Constraining the phase of the estimate

It turns out to be useful to use set the initial phase guesses as the arguments
modulo pi of the complex values of the DFT. In R, $-kn mod n = k$. As a
consequence, if we don't do this then the sign of the amplitude of a cosine
term can get flipped when the phase is negative. Constraining the phases to be
positive seems to have beneficial effects. We'll see if that's true with
negative phase inputs. It probably won't!

It didn't. __BUT__, the problem was that my formulas were __ALL__ wrong: they
should have been $a cos(2 pi x - p)$ rather than $a cos(2 pi x + p)$.

A little phase unwrapping is in order. The unwrap function I chose was

```{r}
unwrap <- function(phases) {
    return((phases + pi) %% (2*pi) - pi)
}
```

That's useful for constraining initial parameter estimates.

## Fallback models

I added two fallback models to each fit: a linear model with just an intercept
term, and one with a slope and intercept.

## Peak finding

It has become increasingly clear that the sorted amplitudes alone won't work as
the criteria for choosing the elements of the basis. If an input frequency is
between two bins of the DFT then both of those bins will have high energy. We
don't actually want to include to adjacent frequencies in the model. The `nls`
part will find the match. We want to move on to the next peak. As a result, I
am incorporating a simple local peak matcher so that the "magnitude order" is
an ordering of local peaks, not all amplitudes.

# 2020 July 24

I think that today's project is to change how I build the models. I had hoped
that `nls` was smart enough to just tweak the hints from the DFT to fit the
cosine basis functions to the output function. Unfortunately, it seems prone to
hitting beats and harmonics just as often as the underlying signals. Therefore,
I think that I am going to build up the models step by step by applying each
successive term to the residuals from the previously accepted terms. The issue
then becomes determining criteria for including terms. The first thing that I
am going to try is a delta criterion approach.
